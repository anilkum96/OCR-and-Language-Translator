{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translation",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3bq8WGXZT4d"
      },
      "source": [
        "## Data Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xqxEJljsxVV"
      },
      "source": [
        "First let start pre processing the data sets. We will first remove all the punctutations and unnecessary characters from the data. After than we will convert each word into their corresponding index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa2vXn4CYfm3",
        "outputId": "95a46d3d-065d-483b-b90f-cd42f06413fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX7CEZOmm4w4"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bihkuggjpj-q"
      },
      "source": [
        "\n",
        "class Lang:\n",
        "    def __init__(self,name):\n",
        "      #Here we are maintaining three dictionaries, one to convert word into index,another index into word and count.\n",
        "      #Also we are maintaining global count of distinct words.\n",
        "      self.name=name\n",
        "      self.word2index={\"<start>\":0,\"<end>\":1}\n",
        "      self.word2count={\"<start>\":0,\"<end>\":0}\n",
        "      self.index2word={0:\"<start>\",1:\"<end>\"}\n",
        "      self.n_count=2\n",
        "    def addsentence(self,sent):\n",
        "      s=sent.split(\" \")\n",
        "      for i in s:\n",
        "        self.addword(i)\n",
        "\n",
        "    #preprocessing the data. Seperating the words into dictionary\n",
        "    def addword(self,word):\n",
        "      if word not in self.word2index:\n",
        "        self.word2index[word]=self.n_count\n",
        "        self.word2count[word]=1\n",
        "        self.index2word[self.n_count]=word\n",
        "        self.n_count+=1\n",
        "      else:\n",
        "        self.word2count[word]+=1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCGv65uAzlil"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def clean(s):\n",
        "    s=s.lower().strip()\n",
        "    s=unicode_to_ascii(s)\n",
        "    s=re.sub(r\"[.!?]+\",r\" \",s)\n",
        "    s=\"<start> \"+s+\" <end>\"\n",
        "    return s\n",
        "def read(source,target,links,reverse=False):\n",
        "  MAX_len=50\n",
        "  with open(links[target]) as f1,open(links[source]) as f2:\n",
        "    pairs=[]\n",
        "    for x,y in zip(f1,f2):\n",
        "      x,y=x.strip(),y.strip()\n",
        "#Because all the other tensors will be padded according to max word length we have to keep an upper cap on max word to limit.\n",
        "      if len(x)<MAX_len and len(y)<MAX_len:\n",
        "        pairs.append([clean(x),clean(y)] )\n",
        "    input_lang=Lang(source)\n",
        "    output_lang=Lang(target)\n",
        "    if reverse:\n",
        "      pairs=[list(reversed(l)) for l in pairs]\n",
        "      input_lang,output_lang=output_lang,input_lang\n",
        "    for pair in pairs:\n",
        "      input_lang.addsentence(pair[0])\n",
        "      output_lang.addsentence(pair[1])\n",
        "    print(input_lang.n_count)\n",
        "    return pairs,input_lang,output_lang\n",
        "\n",
        "def tokenize(pairs,input_lang,output_lang):\n",
        "  input_tensor=list([])\n",
        "  output_tensor=list([])\n",
        "  input_tensor=[[input_lang.word2index[word] for word in i[0].split(\" \")] for i in pairs]\n",
        "  output_tensor=[[output_lang.word2index[word] for word in i[1].split(\" \")] for i in pairs]\n",
        "  # input_tensor=tf.keras.preprocessing.sequence.pad_sequences(input_tensor,padding='post')\n",
        "  # output_tensor=tf.keras.preprocessing.sequence.pad_sequences(output_tensor,padding='post')\n",
        "  \n",
        "  return input_tensor,output_tensor\n",
        "def pad(tensor):\n",
        "  tensor=tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXfVRls2RbRZ",
        "outputId": "536438e5-c529-4c70-f4a2-53a440d2caca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "links={}\n",
        "links[\"hindi\"]=\"/content/drive/My Drive/IITB_hindi.txt\"\n",
        "links[\"english\"]=\"/content/drive/My Drive/IITB_english.txt.en\"\n",
        "pairs,input_lang,output_lang=read(\"hindi\",\"english\",links,reverse=False)\n",
        "print(len(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125614\n",
            "784916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVQlts6VjPgQ"
      },
      "source": [
        "input_tensor,output_tensor=tokenize(pairs,input_lang,output_lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdkUsvyc-45v"
      },
      "source": [
        "input_tensor=pad(input_tensor)\n",
        "output_tensor=pad(output_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjUhgOz4iwJc",
        "outputId": "761479db-cfe7-4f53-c67f-5716503c03c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print(input_tensor[:6])\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    print (\"%d ----> %s\" % (t, lang.index2word[t]))\n",
        "convert(output_lang,output_tensor[1])\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  2  3  4  5  6  7  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  8  6  9  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 10 11 12 13 14 10 15 16  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 10 11 12 13 14 10 17 16  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 18 19  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 18 20 21  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "0 ----> <start>\n",
            "10 ----> एकसरसाइसर\n",
            "5 ----> पहचनीयता\n",
            "11 ----> अनवषक\n",
            "1 ----> <end>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n",
            "0 ----> <start>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1jj23f3MTvC",
        "outputId": "d73b3296-4fed-405e-83b3-f90d049dccfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Training test split\n",
        "input_tensor_train, input_tensor_val, output_tensor_train, output_tensor_val = train_test_split(input_tensor, output_tensor, test_size=0.05)\n",
        "print(len(input_tensor_train), len(output_tensor_train), len(input_tensor_val), len(output_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "745670 745670 39246 39246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgrL0_xbY3AG"
      },
      "source": [
        "All right so far we are done with the pre processing part. Now we will move on to the creating a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvzotgrbZKlN"
      },
      "source": [
        "## The Attention model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rzD0HUXXaG4"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 128\n",
        "iterations=int(BUFFER_SIZE/BATCH_SIZE)\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1000\n",
        "vocab_inp_size = len(input_lang.word2index)+1\n",
        "vocab_tar_size = len(output_lang.word2index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVj8GpVAp4JT"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self,vocab_l,embeding_dim,enc_units,batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.vocab_l=vocab_l\n",
        "    self.embeding_dim=embeding_dim\n",
        "    self.enc_units=enc_units\n",
        "    self.embedding=tf.keras.layers.Embedding(vocab_l,embedding_dim)\n",
        "    self.gru=tf.keras.layers.GRU(self.enc_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self,x,hidden):\n",
        "    x=self.embedding(x)\n",
        "    output,state=self.gru(x,initial_state=hidden)\n",
        "    return output,state\n",
        "  \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size,self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbHuBRC0dP-I",
        "outputId": "0a5298f2-e4f0-4951-bce3-405b10b7cd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,output_tensor_train))\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "example_input_batch,example_target_batch=next(iter(dataset))\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 22), (64, 22)), types: (tf.int32, tf.int32)>\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 22, 64)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef6sKuh4Pua2",
        "outputId": "00d67920-f01a-4a70-8cd1-64459713dd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 22, 64)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9nwMWW1dgQ9"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsEs9tc9di_g",
        "outputId": "4bf188c8-587f-4034-c4c6-9e1c7705f398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 64)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 22, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgCUPh5QVlo"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALiCs73nQakt",
        "outputId": "d0f557a7-8d31-451d-d944-9cf7b9a592d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 172334)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3-N_Em2Qgxy"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljFceWtcQmsj"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([output_lang.word2index['<start>']] * BATCH_SIZE, 1)\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKK-Hx40Q0QX",
        "outputId": "481f8558-53fd-4f4f-c64b-6093ff730f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, output)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp,output, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.7916\n",
            "Epoch 1 Batch 20 Loss 2.4520\n",
            "Epoch 1 Batch 40 Loss 1.8729\n",
            "Epoch 1 Batch 60 Loss 1.8540\n",
            "Epoch 1 Batch 80 Loss 1.5140\n",
            "Epoch 1 Batch 100 Loss 1.3896\n",
            "Epoch 1 Batch 120 Loss 1.3938\n",
            "Epoch 1 Batch 140 Loss 1.5101\n",
            "Epoch 1 Batch 160 Loss 1.5177\n",
            "Epoch 1 Batch 180 Loss 1.5094\n",
            "Epoch 1 Batch 200 Loss 1.4319\n",
            "Epoch 1 Batch 220 Loss 1.5421\n",
            "Epoch 1 Batch 240 Loss 1.3560\n",
            "Epoch 1 Batch 260 Loss 1.5613\n",
            "Epoch 1 Batch 280 Loss 1.5387\n",
            "Epoch 1 Batch 300 Loss 1.3505\n",
            "Epoch 1 Batch 320 Loss 1.3932\n",
            "Epoch 1 Batch 340 Loss 1.4334\n",
            "Epoch 1 Batch 360 Loss 1.4007\n",
            "Epoch 1 Batch 380 Loss 1.5733\n",
            "Epoch 1 Batch 400 Loss 1.4590\n",
            "Epoch 1 Batch 420 Loss 1.5328\n",
            "Epoch 1 Batch 440 Loss 1.4894\n",
            "Epoch 1 Batch 460 Loss 1.4411\n",
            "Epoch 1 Batch 480 Loss 1.3755\n",
            "Epoch 1 Batch 500 Loss 1.3306\n",
            "Epoch 1 Batch 520 Loss 1.3038\n",
            "Epoch 1 Batch 540 Loss 1.3314\n",
            "Epoch 1 Batch 560 Loss 1.3730\n",
            "Epoch 1 Batch 580 Loss 1.5570\n",
            "Epoch 1 Batch 600 Loss 1.5206\n",
            "Epoch 1 Batch 620 Loss 1.5274\n",
            "Epoch 1 Batch 640 Loss 1.5910\n",
            "Epoch 1 Batch 660 Loss 1.5262\n",
            "Epoch 1 Batch 680 Loss 1.2686\n",
            "Epoch 1 Batch 700 Loss 1.2806\n",
            "Epoch 1 Batch 720 Loss 1.3316\n",
            "Epoch 1 Batch 740 Loss 1.4339\n",
            "Epoch 1 Batch 760 Loss 1.5445\n",
            "Epoch 1 Batch 780 Loss 1.4020\n",
            "Epoch 1 Batch 800 Loss 1.3109\n",
            "Epoch 1 Batch 820 Loss 1.3044\n",
            "Epoch 1 Batch 840 Loss 1.4150\n",
            "Epoch 1 Batch 860 Loss 1.3798\n",
            "Epoch 1 Batch 880 Loss 1.2320\n",
            "Epoch 1 Batch 900 Loss 1.3298\n",
            "Epoch 1 Batch 920 Loss 1.1613\n",
            "Epoch 1 Batch 940 Loss 1.2512\n",
            "Epoch 1 Batch 960 Loss 1.3940\n",
            "Epoch 1 Batch 980 Loss 1.3991\n",
            "Epoch 1 Batch 1000 Loss 1.2384\n",
            "Epoch 1 Batch 1020 Loss 1.3331\n",
            "Epoch 1 Batch 1040 Loss 1.3101\n",
            "Epoch 1 Batch 1060 Loss 1.4412\n",
            "Epoch 1 Batch 1080 Loss 1.1503\n",
            "Epoch 1 Batch 1100 Loss 1.2556\n",
            "Epoch 1 Batch 1120 Loss 1.5273\n",
            "Epoch 1 Batch 1140 Loss 1.1359\n",
            "Epoch 1 Batch 1160 Loss 1.2699\n",
            "Epoch 1 Batch 1180 Loss 1.2501\n",
            "Epoch 1 Batch 1200 Loss 1.4406\n",
            "Epoch 1 Batch 1220 Loss 1.0218\n",
            "Epoch 1 Batch 1240 Loss 1.0582\n",
            "Epoch 1 Batch 1260 Loss 1.1346\n",
            "Epoch 1 Batch 1280 Loss 1.1581\n",
            "Epoch 1 Batch 1300 Loss 1.2811\n",
            "Epoch 1 Batch 1320 Loss 1.1611\n",
            "Epoch 1 Batch 1340 Loss 1.2714\n",
            "Epoch 1 Batch 1360 Loss 1.2256\n",
            "Epoch 1 Batch 1380 Loss 1.1594\n",
            "Epoch 1 Batch 1400 Loss 1.2453\n",
            "Epoch 1 Batch 1420 Loss 1.3970\n",
            "Epoch 1 Batch 1440 Loss 1.2575\n",
            "Epoch 1 Batch 1460 Loss 1.3413\n",
            "Epoch 1 Batch 1480 Loss 1.1324\n",
            "Epoch 1 Batch 1500 Loss 1.2022\n",
            "Epoch 1 Batch 1520 Loss 1.1187\n",
            "Epoch 1 Batch 1540 Loss 1.1901\n",
            "Epoch 1 Batch 1560 Loss 1.3361\n",
            "Epoch 1 Batch 1580 Loss 1.2428\n",
            "Epoch 1 Batch 1600 Loss 1.2496\n",
            "Epoch 1 Batch 1620 Loss 1.1605\n",
            "Epoch 1 Batch 1640 Loss 1.3385\n",
            "Epoch 1 Batch 1660 Loss 1.3012\n",
            "Epoch 1 Batch 1680 Loss 1.1611\n",
            "Epoch 1 Batch 1700 Loss 1.2424\n",
            "Epoch 1 Batch 1720 Loss 1.3062\n",
            "Epoch 1 Batch 1740 Loss 1.3516\n",
            "Epoch 1 Batch 1760 Loss 1.2828\n",
            "Epoch 1 Batch 1780 Loss 1.2122\n",
            "Epoch 1 Batch 1800 Loss 1.2025\n",
            "Epoch 1 Batch 1820 Loss 1.3161\n",
            "Epoch 1 Batch 1840 Loss 1.2804\n",
            "Epoch 1 Batch 1860 Loss 1.4283\n",
            "Epoch 1 Batch 1880 Loss 1.0667\n",
            "Epoch 1 Batch 1900 Loss 1.3697\n",
            "Epoch 1 Batch 1920 Loss 1.2672\n",
            "Epoch 1 Batch 1940 Loss 1.3087\n",
            "Epoch 1 Batch 1960 Loss 1.2687\n",
            "Epoch 1 Batch 1980 Loss 1.2760\n",
            "Epoch 1 Batch 2000 Loss 1.2586\n",
            "Epoch 1 Batch 2020 Loss 1.2612\n",
            "Epoch 1 Batch 2040 Loss 1.2218\n",
            "Epoch 1 Batch 2060 Loss 1.2017\n",
            "Epoch 1 Batch 2080 Loss 0.9634\n",
            "Epoch 1 Batch 2100 Loss 1.3770\n",
            "Epoch 1 Batch 2120 Loss 1.2229\n",
            "Epoch 1 Batch 2140 Loss 1.1747\n",
            "Epoch 1 Batch 2160 Loss 1.2215\n",
            "Epoch 1 Batch 2180 Loss 1.1886\n",
            "Epoch 1 Batch 2200 Loss 1.2725\n",
            "Epoch 1 Batch 2220 Loss 1.1232\n",
            "Epoch 1 Batch 2240 Loss 1.2237\n",
            "Epoch 1 Batch 2260 Loss 1.1094\n",
            "Epoch 1 Batch 2280 Loss 1.3163\n",
            "Epoch 1 Batch 2300 Loss 1.2141\n",
            "Epoch 1 Batch 2320 Loss 1.1532\n",
            "Epoch 1 Batch 2340 Loss 1.2086\n",
            "Epoch 1 Batch 2360 Loss 1.1046\n",
            "Epoch 1 Batch 2380 Loss 1.2549\n",
            "Epoch 1 Batch 2400 Loss 1.1261\n",
            "Epoch 1 Batch 2420 Loss 1.1168\n",
            "Epoch 1 Batch 2440 Loss 1.3249\n",
            "Epoch 1 Batch 2460 Loss 1.1624\n",
            "Epoch 1 Batch 2480 Loss 1.2121\n",
            "Epoch 1 Batch 2500 Loss 1.2714\n",
            "Epoch 1 Batch 2520 Loss 1.1117\n",
            "Epoch 1 Batch 2540 Loss 1.0975\n",
            "Epoch 1 Batch 2560 Loss 1.3313\n",
            "Epoch 1 Batch 2580 Loss 1.2629\n",
            "Epoch 1 Batch 2600 Loss 1.2556\n",
            "Epoch 1 Batch 2620 Loss 1.1697\n",
            "Epoch 1 Batch 2640 Loss 1.2533\n",
            "Epoch 1 Batch 2660 Loss 1.1384\n",
            "Epoch 1 Batch 2680 Loss 1.2130\n",
            "Epoch 1 Batch 2700 Loss 1.1872\n",
            "Epoch 1 Batch 2720 Loss 1.0828\n",
            "Epoch 1 Batch 2740 Loss 1.1235\n",
            "Epoch 1 Batch 2760 Loss 1.1718\n",
            "Epoch 1 Batch 2780 Loss 1.1826\n",
            "Epoch 1 Batch 2800 Loss 1.0769\n",
            "Epoch 1 Batch 2820 Loss 1.3821\n",
            "Epoch 1 Batch 2840 Loss 1.0667\n",
            "Epoch 1 Batch 2860 Loss 1.1898\n",
            "Epoch 1 Batch 2880 Loss 1.2623\n",
            "Epoch 1 Batch 2900 Loss 1.2765\n",
            "Epoch 1 Batch 2920 Loss 1.2969\n",
            "Epoch 1 Batch 2940 Loss 1.2134\n",
            "Epoch 1 Batch 2960 Loss 1.3139\n",
            "Epoch 1 Batch 2980 Loss 1.2661\n",
            "Epoch 1 Batch 3000 Loss 1.1567\n",
            "Epoch 1 Batch 3020 Loss 1.1270\n",
            "Epoch 1 Batch 3040 Loss 1.2406\n",
            "Epoch 1 Batch 3060 Loss 1.0516\n",
            "Epoch 1 Batch 3080 Loss 1.1442\n",
            "Epoch 1 Batch 3100 Loss 1.2072\n",
            "Epoch 1 Batch 3120 Loss 1.1293\n",
            "Epoch 1 Batch 3140 Loss 1.1643\n",
            "Epoch 1 Batch 3160 Loss 1.3650\n",
            "Epoch 1 Batch 3180 Loss 1.3422\n",
            "Epoch 1 Batch 3200 Loss 1.2278\n",
            "Epoch 1 Batch 3220 Loss 1.2775\n",
            "Epoch 1 Batch 3240 Loss 1.3384\n",
            "Epoch 1 Batch 3260 Loss 1.0817\n",
            "Epoch 1 Batch 3280 Loss 1.1969\n",
            "Epoch 1 Batch 3300 Loss 1.1282\n",
            "Epoch 1 Batch 3320 Loss 1.0734\n",
            "Epoch 1 Batch 3340 Loss 1.1342\n",
            "Epoch 1 Batch 3360 Loss 1.2375\n",
            "Epoch 1 Batch 3380 Loss 1.2155\n",
            "Epoch 1 Batch 3400 Loss 1.1299\n",
            "Epoch 1 Batch 3420 Loss 1.3039\n",
            "Epoch 1 Batch 3440 Loss 1.2497\n",
            "Epoch 1 Batch 3460 Loss 1.1753\n",
            "Epoch 1 Batch 3480 Loss 1.3583\n",
            "Epoch 1 Batch 3500 Loss 1.0607\n",
            "Epoch 1 Batch 3520 Loss 1.1459\n",
            "Epoch 1 Batch 3540 Loss 1.2441\n",
            "Epoch 1 Batch 3560 Loss 1.2567\n",
            "Epoch 1 Batch 3580 Loss 1.0232\n",
            "Epoch 1 Batch 3600 Loss 1.2189\n",
            "Epoch 1 Batch 3620 Loss 1.1900\n",
            "Epoch 1 Batch 3640 Loss 1.0777\n",
            "Epoch 1 Batch 3660 Loss 1.2675\n",
            "Epoch 1 Batch 3680 Loss 1.0606\n",
            "Epoch 1 Batch 3700 Loss 1.0986\n",
            "Epoch 1 Batch 3720 Loss 1.1697\n",
            "Epoch 1 Batch 3740 Loss 1.2257\n",
            "Epoch 1 Batch 3760 Loss 1.2120\n",
            "Epoch 1 Batch 3780 Loss 1.1918\n",
            "Epoch 1 Batch 3800 Loss 0.9065\n",
            "Epoch 1 Batch 3820 Loss 1.1778\n",
            "Epoch 1 Batch 3840 Loss 1.2796\n",
            "Epoch 1 Batch 3860 Loss 1.1553\n",
            "Epoch 1 Batch 3880 Loss 1.2277\n",
            "Epoch 1 Batch 3900 Loss 1.1498\n",
            "Epoch 1 Batch 3920 Loss 1.0929\n",
            "Epoch 1 Batch 3940 Loss 1.1302\n",
            "Epoch 1 Batch 3960 Loss 1.1024\n",
            "Epoch 1 Batch 3980 Loss 1.0263\n",
            "Epoch 1 Batch 4000 Loss 1.0620\n",
            "Epoch 1 Batch 4020 Loss 1.1153\n",
            "Epoch 1 Batch 4040 Loss 1.1949\n",
            "Epoch 1 Batch 4060 Loss 1.0613\n",
            "Epoch 1 Batch 4080 Loss 1.1982\n",
            "Epoch 1 Batch 4100 Loss 0.9408\n",
            "Epoch 1 Batch 4120 Loss 1.0657\n",
            "Epoch 1 Batch 4140 Loss 1.0937\n",
            "Epoch 1 Batch 4160 Loss 1.1666\n",
            "Epoch 1 Batch 4180 Loss 1.2516\n",
            "Epoch 1 Batch 4200 Loss 1.0783\n",
            "Epoch 1 Batch 4220 Loss 1.1344\n",
            "Epoch 1 Batch 4240 Loss 1.0961\n",
            "Epoch 1 Batch 4260 Loss 1.1030\n",
            "Epoch 1 Batch 4280 Loss 1.0564\n",
            "Epoch 1 Batch 4300 Loss 1.0515\n",
            "Epoch 1 Batch 4320 Loss 1.0978\n",
            "Epoch 1 Batch 4340 Loss 1.0725\n",
            "Epoch 1 Batch 4360 Loss 1.1339\n",
            "Epoch 1 Batch 4380 Loss 1.1222\n",
            "Epoch 1 Batch 4400 Loss 1.2046\n",
            "Epoch 1 Batch 4420 Loss 1.0408\n",
            "Epoch 1 Batch 4440 Loss 1.0952\n",
            "Epoch 1 Batch 4460 Loss 1.0699\n",
            "Epoch 1 Batch 4480 Loss 1.0472\n",
            "Epoch 1 Batch 4500 Loss 1.0266\n",
            "Epoch 1 Batch 4520 Loss 1.0119\n",
            "Epoch 1 Batch 4540 Loss 1.2278\n",
            "Epoch 1 Batch 4560 Loss 0.9938\n",
            "Epoch 1 Batch 4580 Loss 1.0566\n",
            "Epoch 1 Batch 4600 Loss 1.0668\n",
            "Epoch 1 Batch 4620 Loss 1.1805\n",
            "Epoch 1 Batch 4640 Loss 1.2124\n",
            "Epoch 1 Batch 4660 Loss 1.0993\n",
            "Epoch 1 Batch 4680 Loss 1.2816\n",
            "Epoch 1 Batch 4700 Loss 1.2438\n",
            "Epoch 1 Batch 4720 Loss 1.0910\n",
            "Epoch 1 Batch 4740 Loss 1.0329\n",
            "Epoch 1 Batch 4760 Loss 1.1438\n",
            "Epoch 1 Batch 4780 Loss 1.2020\n",
            "Epoch 1 Batch 4800 Loss 1.1662\n",
            "Epoch 1 Batch 4820 Loss 1.0142\n",
            "Epoch 1 Batch 4840 Loss 1.1121\n",
            "Epoch 1 Batch 4860 Loss 1.1519\n",
            "Epoch 1 Batch 4880 Loss 1.1270\n",
            "Epoch 1 Batch 4900 Loss 1.1477\n",
            "Epoch 1 Batch 4920 Loss 0.9760\n",
            "Epoch 1 Batch 4940 Loss 1.0571\n",
            "Epoch 1 Batch 4960 Loss 1.0658\n",
            "Epoch 1 Batch 4980 Loss 1.1199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY-jOvLPlR5L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}